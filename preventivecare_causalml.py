# -*- coding: utf-8 -*-
"""PreventiveCare-CausalML .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-00ynLj3Ll1NBEn-_zvLxofB4K1LkBmZ
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Set random seed for reproducibility
np.random.seed(42)

# =============================================================================
# Data Simulation
# =============================================================================
# Simulation parameters
N = 1000
MONTHS = 12
PROGRAMS = 3
program_effects = {'A': 0.12, 'B': 0.08, 'C': 0.15}

# Generate baseline characteristics
prior_physical = np.random.binomial(1, 0.4, size=N)
office_visits = np.clip(np.random.poisson(6, size=N), 0, 12)

# Initialize data structures
outreach_history = np.zeros((N, MONTHS, PROGRAMS))
completed = np.zeros(N, dtype=int)
completion_month = np.zeros(N, dtype=int)

# Outreach probability function
def outreach_probability(program, month, had_physical, office_visits, prev_outreach):
    base_probs = {0: 0.10, 1: 0.08, 2: 0.12}
    factors = {
        'month': 1.0 + 0.2 * np.sin((month - 2) * np.pi / 6),
        'physical': 0.8 if had_physical else 1.2,
        'visits': 1.3 - (0.05 * office_visits),
        'history': 0.7 if prev_outreach > 0 else 1.0
    }
    prob = base_probs[program] * np.prod(list(factors.values()))
    return np.clip(prob, 0.01, 0.30)

# Simulation loop
base_hazard = np.linspace(0.12, 0.04, MONTHS)
for month in range(MONTHS):
    active = (completed == 0)
    if not active.any(): continue

    # Simulate outreach
    for program in range(PROGRAMS):
        for i in range(N):
            if not active[i]: continue
            prev_outreach = np.sum(outreach_history[i, :month, program])
            prob = outreach_probability(program, month, prior_physical[i],
                                       office_visits[i], prev_outreach)
            if np.random.rand() < prob:
                outreach_history[i, month, program] = 1

    # Simulate completions
    for i in range(N):
        if not active[i]: continue
        prob = base_hazard[month]
        if prior_physical[i]: prob *= 1.3
        prob *= 0.7 + (0.05 * office_visits[i])
        for prog in range(PROGRAMS):
            if outreach_history[i, month, prog]:
                prob += program_effects[chr(65 + prog)]
        if np.random.rand() < np.clip(prob, 0, 0.95):
            completed[i] = 1
            completion_month[i] = month + 1

# Create datasets
member_data = pd.DataFrame({
    'member_id': range(N),
    'prior_physical': prior_physical,
    'office_visits': office_visits,
    'completed_checkup': completed,
    'completion_month': completion_month
})
member_data['A_outreach'] = outreach_history[:, :, 0].sum(axis=1) > 0
member_data['B_outreach'] = outreach_history[:, :, 1].sum(axis=1) > 0
member_data['C_outreach'] = outreach_history[:, :, 2].sum(axis=1) > 0

person_month_data = []
for i in range(N):
    last_month = MONTHS if completion_month[i] == 0 else completion_month[i]
    for m in range(1, last_month + 1):
        entry = {
            'member_id': i,
            'month': m,
            'prior_physical': prior_physical[i],
            'office_visits': office_visits[i],
            'completed': (completion_month[i] == m),
            'prev_A': outreach_history[i, :m-1, 0].sum() > 0,
            'prev_B': outreach_history[i, :m-1, 1].sum() > 0,
            'prev_C': outreach_history[i, :m-1, 2].sum() > 0,
            'A_this_month': outreach_history[i, m-1, 0],
            'B_this_month': outreach_history[i, m-1, 1],
            'C_this_month': outreach_history[i, m-1, 2]
        }
        person_month_data.append(entry)
person_month_df = pd.DataFrame(person_month_data)

# =============================================================================
# Analysis Pipeline
# =============================================================================
def plot_weight_distribution(weights):
    plt.figure(figsize=(10, 6))
    sns.histplot(weights, bins=50, kde=True)
    plt.axvline(weights.mean(), color='r', linestyle='--', label='Mean')
    plt.title('Stabilized Inverse Probability Weight Distribution')
    plt.xlabel('Weight Value')
    plt.ylabel('Density')
    plt.legend()
    plt.show()

def calculate_ipw(df):
    df_copy = df.copy()
    df_copy['w_A'] = df_copy['w_B'] = df_copy['w_C'] = 1.0

    for m in range(1, MONTHS + 1):
        month_data = df_copy[df_copy['month'] == m].copy()
        if month_data.empty: continue

        predictors = ['prior_physical', 'office_visits', 'month',
                     'prev_A', 'prev_B', 'prev_C',
                     'prior_physical_office_visits']

        for prog in ['A', 'B', 'C']:
            col = f'{prog}_this_month'
            trt = month_data[col]
            if trt.mean() in (0, 1): continue

            model = LogisticRegression(penalty='l2', C=0.1, solver='liblinear', max_iter=5000)
            model.fit(month_data[predictors], trt)

            prob = np.clip(model.predict_proba(month_data[predictors])[:, 1], 0.2, 0.8)
            marg_prob = trt.mean()

            weights = np.where(trt == 1,
                             marg_prob / prob,
                             (1 - marg_prob) / (1 - prob))
            df_copy.loc[month_data.index, f'w_{prog}'] = weights / weights.mean()

    df_copy['cumulative_w'] = (df_copy[['w_A', 'w_B', 'w_C']].prod(axis=1) ** 0.9).groupby(df_copy['member_id']).cumprod()
    Q1, Q3 = df_copy['cumulative_w'].quantile([0.25, 0.75])
    df_copy['ipw_trimmed'] = np.clip(df_copy['cumulative_w'], Q1-1.5*(Q3-Q1), Q3+1.5*(Q3-Q1))
    df_copy['ipw_trimmed'] /= df_copy['ipw_trimmed'].mean()

    return df_copy

def doubly_robust_estimate(df, n_boot=200):
    """Doubly robust estimation with proper prediction alignment"""
    # Fit outcome model on original data
    outcome_model = GradientBoostingClassifier()
    X = df[['A_this_month', 'B_this_month', 'C_this_month',
           'prior_physical', 'office_visits']]
    outcome_model.fit(X, df['completed'])

    # Initialize results storage
    dr_effects = {'A': [], 'B': [], 'C': []}

    for _ in range(n_boot):
        # Resample with replacement
        sample = df.sample(frac=1, replace=True).reset_index(drop=True)

        # Calculate IPW weights for bootstrap sample
        sample_ipw = calculate_ipw(sample)

        # Merge weights ensuring 1:1 match
        sample = sample.merge(
            sample_ipw[['member_id', 'month', 'ipw_trimmed']],
            on=['member_id', 'month'],
            how='left',
            suffixes=('', '_ipw')
        )

        # Get predictions for THIS bootstrap sample's features
        X_sample = sample[['A_this_month', 'B_this_month', 'C_this_month',
                          'prior_physical', 'office_visits']]
        predictions = outcome_model.predict_proba(X_sample)[:, 1]

        for prog in ['A', 'B', 'C']:
            treated = sample[f'{prog}_this_month'] == 1
            control = ~treated

            # Handle potential zero/inf weights
            weights = sample['ipw_trimmed'].replace([np.inf, -np.inf], np.nan).fillna(1)

            # Calculate components with aligned predictions
            component1 = ((sample['completed'] - predictions) * treated / weights).mean()
            component2 = ((sample['completed'] - predictions) * control / weights).mean()
            component3 = predictions[treated].mean() - predictions[control].mean()

            dr_effect = component1 - component2 + component3
            dr_effects[prog].append(dr_effect)

    # Calculate final estimates and CIs
    return {
        k: (np.nanmean(v),
           (np.nanpercentile(v, 2.5), np.nanpercentile(v, 97.5)))
        for k, v in dr_effects.items()
    }

# =============================================================================
# Execute Analysis
# =============================================================================
# Add interaction terms
for df in [person_month_df, member_data]:
    df['prior_physical_office_visits'] = df['prior_physical'] * df['office_visits']

# Calculate weights
person_month_df_ipw = calculate_ipw(person_month_df)
plot_weight_distribution(person_month_df_ipw['ipw_trimmed'])

# Merge weights
final_weights = person_month_df_ipw.groupby('member_id')['ipw_trimmed'].last().reset_index()
member_data = member_data.merge(final_weights, on='member_id', how='left')

# Fit models
X_msm = sm.add_constant(person_month_df_ipw[['A_this_month', 'B_this_month', 'C_this_month',
                                           'prior_physical', 'office_visits']])
msm_model = sm.GLM(person_month_df_ipw['completed'], X_msm,
                 freq_weights=person_month_df_ipw['ipw_trimmed'],
                 family=sm.families.Binomial()).fit(cov_type='HC0')

dr_results = doubly_robust_estimate(person_month_df_ipw)

# Fit naive model
member_data[['A_outreach', 'B_outreach', 'C_outreach']] = member_data[['A_outreach', 'B_outreach', 'C_outreach']].astype(int)
naive_model = sm.Logit(member_data['completed_checkup'],
                      sm.add_constant(member_data[['A_outreach', 'B_outreach', 'C_outreach',
                                                 'prior_physical', 'office_visits']])).fit(disp=0)

# =============================================================================
# Results Presentation
# =============================================================================
def or_to_percent(or_val, baseline):
    odds = baseline / (1 - baseline)
    return (or_val*odds / (1 + or_val*odds) - baseline) * 100

baseline_risk = person_month_df_ipw['completed'].mean()
true_effects = {'A': 12.0, 'B': 8.0, 'C': 15.0}

# Prepare results
results = []
for prog in ['A', 'B', 'C']:
    # MSM results
    msm_or = np.exp(msm_model.params[f'{prog}_this_month'])
    msm_ci = np.exp(msm_model.conf_int().loc[f'{prog}_this_month'])
    msm_effect = or_to_percent(msm_or, baseline_risk)
    msm_lower = or_to_percent(msm_ci[0], baseline_risk)
    msm_upper = or_to_percent(msm_ci[1], baseline_risk)

    # DR results
    dr_mean, dr_ci = dr_results[prog]

    # Naive results
    naive_or = np.exp(naive_model.params[f'{prog}_outreach'])
    naive_ci = np.exp(naive_model.conf_int().loc[f'{prog}_outreach'])
    naive_effect = or_to_percent(naive_or, baseline_risk)

    results.append({
        'Program': prog,
        'True': true_effects[prog],
        'MSM': f"{msm_effect:.1f}% ({msm_lower:.1f}-{msm_upper:.1f})",
        'DR': f"{dr_mean*100:.1f}% ({dr_ci[0]*100:.1f}-{dr_ci[1]*100:.1f})",
        'Naive': f"{naive_effect:.1f}%"
    })

# Print formatted table
print("\nComparative Program Effects (Percentage Point Differences)")
print("==============================================================")
print(f"{'Program':<8} | {'True':<8} | {'MSM (95% CI)':<20} | {'DR (95% CI)':<20} | {'Naive':<10}")
print("==============================================================")
for res in results:
    print(f"{res['Program']:<8} | {res['True']:>6.1f}% | {res['MSM']:>20} | {res['DR']:>20} | {res['Naive']:>10}")
print("==============================================================")
print(f"Baseline completion rate: {baseline_risk:.1%}")

# Plot effects
plt.figure(figsize=(12, 6))
x = np.arange(3)
width = 0.2

for i, (method, color) in enumerate(zip(['MSM', 'DR', 'Naive'], ['#1f77b4', '#ff7f0e', '#2ca02c'])):
    effects = [float(res[method].split('%')[0]) for res in results]
    plt.bar(x + width*i, effects, width, label=method, color=color)

plt.axhline(0, color='black', linewidth=0.8)
plt.xticks(x + width, ['A', 'B', 'C'])
plt.ylabel('Percentage Point Difference')
plt.title('Program Effect Estimates Comparison')
plt.legend()
plt.tight_layout()
plt.show()